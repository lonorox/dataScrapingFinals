# Web Scraping System

A powerful, scalable web scraping system built with Python that supports multiple data sources including news sites, RSS feeds, and blogs. The system uses a Master-Worker architecture with multiprocessing for efficient concurrent scraping operations.

## üåü Features

- **Multi-Source Scraping**: Support for news sites, RSS feeds, and blogs
- **Scalable Architecture**: Master-Worker pattern with configurable worker pools
- **Design Patterns**: Factory and Strategy patterns for extensible scraping
- **Rate Limiting**: Respectful scraping with configurable request throttling
- **Proxy Support**: Anonymous scraping with proxy rotation
- **Data Analysis**: Comprehensive reporting and analytics
- **Interactive CLI**: User-friendly command-line interface
- **Database Integration**: SQLite storage with efficient data management
- **Error Handling**: Robust retry logic and graceful error recovery

## üöÄ Quick Start

### Prerequisites

- Python 3.8 or higher
- pip (Python package installer)

### Installation

1. **Navigate to project directory**
   ```bash
   cd dataScrapingFinals
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   
   # On Windows
   venv\Scripts\activate
   
   # On macOS/Linux
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Verify installation**
   ```bash
   python main.py --help
   ```

### Basic Usage

1. **Run interactive mode**
   ```bash
   python main.py
   ```

2. **Run all tasks automatically**
   ```bash
   python main.py --auto
   ```

3. **Run specific content type**
   ```bash
   python main.py --type news
   python main.py --type rss
   python main.py --type blog
   ```

4. **Generate reports**
   ```bash
   python main.py --report
   ```

## üìä Supported Data Sources

### News Sources
- **BBC News** (`bbc.com`) - Latest news articles with categories
- **Fox News** (`foxnews.com`) - News articles and headlines

### RSS Sources
- **NPR** (`npr.org`) - RSS feed articles with search filtering

### Blog Sources
- **TechCrunch** (`techcrunch.com`) - Technology blog posts

## üèóÔ∏è Architecture

The system follows a **Master-Worker pattern** with **multiprocessing** for concurrent scraping operations:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CLI Interface ‚îÇ    ‚îÇ   Configuration ‚îÇ    ‚îÇ   Data Storage  ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Interactive   ‚îÇ    ‚îÇ ‚Ä¢ Task Config   ‚îÇ    ‚îÇ ‚Ä¢ SQLite DB     ‚îÇ
‚îÇ ‚Ä¢ Command Line  ‚îÇ    ‚îÇ ‚Ä¢ Worker Config ‚îÇ    ‚îÇ ‚Ä¢ JSON Files    ‚îÇ
‚îÇ ‚Ä¢ Batch Mode    ‚îÇ    ‚îÇ ‚Ä¢ Proxy Config  ‚îÇ    ‚îÇ ‚Ä¢ CSV Reports   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Master Node   ‚îÇ
                    ‚îÇ                 ‚îÇ
                    ‚îÇ ‚Ä¢ Task Queue    ‚îÇ
                    ‚îÇ ‚Ä¢ Result Queue  ‚îÇ
                    ‚îÇ ‚Ä¢ Worker Mgmt   ‚îÇ
                    ‚îÇ ‚Ä¢ Stats Tracking‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Worker Pool   ‚îÇ
                    ‚îÇ                 ‚îÇ
                    ‚îÇ ‚Ä¢ Worker 1      ‚îÇ
                    ‚îÇ ‚Ä¢ Worker 2      ‚îÇ
                    ‚îÇ ‚Ä¢ Worker N      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Scraper Types  ‚îÇ
                    ‚îÇ                 ‚îÇ
                    ‚îÇ ‚Ä¢ News Scraper  ‚îÇ
                    ‚îÇ ‚Ä¢ RSS Scraper   ‚îÇ
                    ‚îÇ ‚Ä¢ Blog Scraper  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Design Patterns

- **Factory Pattern**: Dynamic scraper creation based on task type
- **Strategy Pattern**: Site-specific scraping strategies
- **Master-Worker Pattern**: Distributed task processing
- **Observer Pattern**: Real-time monitoring and status tracking

## üìÅ Project Structure

```
dataScrapingFinals/
‚îú‚îÄ‚îÄ main.py                 # Main entry point
‚îú‚îÄ‚îÄ config.json            # Configuration file
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ docs/                  # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md    # System architecture
‚îÇ   ‚îî‚îÄ‚îÄ user_guide.md      # User guide
‚îú‚îÄ‚îÄ src/                   # Source code
‚îÇ   ‚îú‚îÄ‚îÄ cli/              # Command-line interface
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/         # Scraping components
‚îÇ   ‚îú‚îÄ‚îÄ data/             # Data models and processing
‚îÇ   ‚îú‚îÄ‚îÄ analysis/         # Data analysis and reporting
‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Utilities and helpers
‚îú‚îÄ‚îÄ data_output/          # Output data
‚îÇ   ‚îú‚îÄ‚îÄ raw/              # Raw scraped data
‚îÇ   ‚îú‚îÄ‚îÄ processed/        # Processed data
‚îÇ   ‚îî‚îÄ‚îÄ reports/          # Generated reports
‚îî‚îÄ‚îÄ tests/                # Test suite
```

## ‚öôÔ∏è Configuration

The system is configured via `config.json`:

```json
{
  "min_workers": 2,
  "max_workers": 5,
  "tasks": [
    {
      "priority": 1,
      "url": "https://www.bbc.com/",
      "type": "news"
    },
    {
      "priority": 2,
      "url": "",
      "type": "rss",
      "search_word": "technology"
    }
  ],
  "userAgents": [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36..."
  ],
  "proxies": [
    "http://proxy1:port",
    "http://proxy2:port"
  ]
}
```

### Configuration Options

- **Worker Pool**: Configure min/max worker processes
- **Tasks**: Define scraping tasks with priority and type
- **Network**: User agent rotation and proxy support
- **Rate Limiting**: Request throttling settings

## üìà Usage Examples

### Interactive Mode
```bash
python main.py
# Navigate through the menu system
```

### Command Line Mode
```bash
# Run all configured tasks
python main.py --auto

# Run specific content type
python main.py --type news

# Generate comprehensive report
python main.py --report

# Use custom configuration
python main.py --config my_config.json
```

### Custom Configuration
```bash
# Configure news scraping
echo '{
  "min_workers": 2,
  "max_workers": 3,
  "tasks": [
    {
      "priority": 1,
      "url": "https://www.bbc.com/",
      "type": "news"
    }
  ]
}' > config.json

# Run scraping
python main.py --type news
```

## üìä Reports & Analytics

The system generates comprehensive reports:

### Available Reports
- **Data Overview**: Summary of scraped data
- **Performance Analytics**: Scraping performance metrics
- **Task Summary**: Task configuration and status
- **HTML Reports**: Interactive web-based reports

### Output Formats
- **JSON**: Raw data structure
- **CSV**: Tabular format for analysis
- **Excel**: Formatted reports with charts
- **HTML**: Interactive web reports

### Analysis Features
- Statistical analysis of scraped content
- Trend analysis and temporal patterns
- Source distribution and popularity metrics
- Content classification and categorization

## üîß Advanced Features

### Custom Scrapers
```python
from src.scrapers.ScraperFactory import Scraper
from src.data.models import Article

class CustomScraper(Scraper):
    def scrape(self, url: str) -> List[Article]:
        # Implement your scraping logic
        articles = []
        # ... scraping code ...
        return articles
```

### Database Integration
```python
from src.data.database import Database

db = Database()
articles = db.get_articles_by_source("BBC")
```

### Scheduled Scraping
```bash
# Add to crontab for automated scraping
0 */6 * * * cd /path/to/project && python main.py --auto
```

## üß™ Testing

Run the comprehensive test suite:

```bash
# Run all tests
python main.py
# Select "Run Tests"

# Or directly with pytest
python -m pytest tests/ -v

# Or with unittest
python -m unittest discover tests/
```

## üìö Documentation

- **[Architecture Guide](docs/architecture.md)**: Detailed system architecture and design patterns
- **[User Guide](docs/user_guide.md)**: Comprehensive usage instructions and examples

## üõ†Ô∏è Troubleshooting

### Common Issues

1. **Import Errors**
   ```bash
   # Ensure virtual environment is activated
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows
   ```

2. **Permission Errors**
   ```bash
   # Check and fix permissions
   chmod 755 data_output/
   chmod 644 data_output/raw/*
   ```

3. **Network Errors**
   ```bash
   # Test connectivity
   curl -I https://www.bbc.com/
   
   # Check proxy settings
   python main.py
   # Select "Configure Settings" ‚Üí "Proxy Settings"
   ```

4. **Worker Failures**
   ```bash
   # Check logs
   tail -f logs.log
   
   # Verify worker settings
   python main.py
   # Select "Configure Settings" ‚Üí "Worker Settings"
   ```

### Debug Mode
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Adding New Scrapers
1. Create new scraper class implementing `Scraper` interface
2. Add to `ScraperFactory.create_scraper()` method
3. Update configuration schema if needed
4. Add tests for new scraper

### Adding New Data Sources
1. Implement site-specific strategy
2. Add to `ScrapingContext` strategies
3. Update URL validation logic
4. Test with sample data

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Built with Python 3.8+
- Uses BeautifulSoup for HTML parsing
- Selenium for dynamic content scraping
- Scrapy framework for blog scraping
- SQLite for data storage
- Matplotlib and Seaborn for visualizations

## üìû Support

- **Documentation**: Check the [docs/](docs/) directory
- **Issues**: Report bugs and feature requests
- **Logs**: Check `logs.log` for detailed error information
- **System Status**: Use CLI to check system health

---

**Happy Scraping! üï∑Ô∏èüìä** 